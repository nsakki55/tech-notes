{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231f04e6ce3d42768815a1300918aefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/641k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce6d9b01d4e4e1286a9603f4b82671c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191bec7f343f43d9846b91210612a60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c32f4d66d0f4424b9f21ce84ac9e8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€\n",
    "dataset = load_dataset(\"llm-book/ner-wikipedia-dataset\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 4274\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 534\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 535\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'curid': '3638038',\n",
      "  'entities': [{'name': 'ã•ãã‚‰å­¦é™¢', 'span': [0, 5], 'type': 'ãã®ä»–ã®çµ„ç¹”å'},\n",
      "               {'name': 'Ciao Smiles', 'span': [6, 17], 'type': 'ãã®ä»–ã®çµ„ç¹”å'}],\n",
      "  'text': 'ã•ãã‚‰å­¦é™¢ã€Ciao Smilesã®ãƒ¡ãƒ³ãƒãƒ¼ã€‚'},\n",
      " {'curid': '1729527',\n",
      "  'entities': [{'name': 'ãƒ¬ã‚¯ãƒ¬ã‚¢ãƒ†ã‚£ãƒ¼ãƒœãƒ»ã‚¦ã‚§ãƒ«ãƒ', 'span': [17, 30], 'type': 'ãã®ä»–ã®çµ„ç¹”å'},\n",
      "               {'name': 'ãƒ—ãƒªãƒ¡ãƒ¼ãƒ©ãƒ»ãƒ‡ã‚£ãƒ“ã‚·ã‚ªãƒ³', 'span': [32, 44], 'type': 'ãã®ä»–ã®çµ„ç¹”å'}],\n",
      "  'text': '2008å¹´10æœˆ5æ—¥ã€ã‚¢ã‚¦ã‚§ãƒ¼ã§ã®ãƒ¬ã‚¯ãƒ¬ã‚¢ãƒ†ã‚£ãƒ¼ãƒœãƒ»ã‚¦ã‚§ãƒ«ãƒæˆ¦ã§ãƒ—ãƒªãƒ¡ãƒ¼ãƒ©ãƒ»ãƒ‡ã‚£ãƒ“ã‚·ã‚ªãƒ³ã§ã®åˆå¾—ç‚¹ã‚’æ±ºã‚ãŸã€‚'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(list(dataset[\"train\"])[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>äººå</th>\n",
       "      <td>2394</td>\n",
       "      <td>299</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>æ³•äººå</th>\n",
       "      <td>2006</td>\n",
       "      <td>231</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>åœ°å</th>\n",
       "      <td>1769</td>\n",
       "      <td>184</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>æ”¿æ²»çš„çµ„ç¹”å</th>\n",
       "      <td>953</td>\n",
       "      <td>121</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>è£½å“å</th>\n",
       "      <td>934</td>\n",
       "      <td>123</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>æ–½è¨­å</th>\n",
       "      <td>868</td>\n",
       "      <td>103</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ãã®ä»–ã®çµ„ç¹”å</th>\n",
       "      <td>852</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ã‚¤ãƒ™ãƒ³ãƒˆå</th>\n",
       "      <td>831</td>\n",
       "      <td>85</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>åˆè¨ˆ</th>\n",
       "      <td>10607</td>\n",
       "      <td>1245</td>\n",
       "      <td>1333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train  validation  test\n",
       "äººå        2394         299   287\n",
       "æ³•äººå       2006         231   248\n",
       "åœ°å        1769         184   204\n",
       "æ”¿æ²»çš„çµ„ç¹”å     953         121   106\n",
       "è£½å“å        934         123   158\n",
       "æ–½è¨­å        868         103   137\n",
       "ãã®ä»–ã®çµ„ç¹”å    852          99   100\n",
       "ã‚¤ãƒ™ãƒ³ãƒˆå      831          85    93\n",
       "åˆè¨ˆ       10607        1245  1333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def count_label_occurrences(dataset: Dataset) -> dict[str, int]:\n",
    "    \"\"\"å›ºæœ‰è¡¨ç¾ã‚¿ã‚¤ãƒ—ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\"\"\"\n",
    "    # å„äº‹ä¾‹ã‹ã‚‰å›ºæœ‰è¡¨ç¾ã‚¿ã‚¤ãƒ—ã‚’æŠ½å‡ºã—ãŸlistã‚’ä½œæˆã™ã‚‹\n",
    "    entities = [\n",
    "        e[\"type\"] for data in dataset for e in data[\"entities\"]\n",
    "    ]\n",
    "\n",
    "    # ãƒ©ãƒ™ãƒ«ã®å‡ºç¾å›æ•°ãŒå¤šã„é †ã«ä¸¦ã³æ›¿ãˆã‚‹\n",
    "    label_counts = dict(Counter(entities).most_common())\n",
    "    return label_counts\n",
    "\n",
    "label_counts_dict = {}\n",
    "for split in dataset: # å„åˆ†å‰²ã‚»ãƒƒãƒˆã‚’å‡¦ç†ã™ã‚‹\n",
    "    label_counts_dict[split] = count_label_occurrences(dataset[split])\n",
    "# DataFrameå½¢å¼ã§è¡¨ç¤ºã™ã‚‹\n",
    "df = pd.DataFrame(label_counts_dict)\n",
    "df.loc[\"åˆè¨ˆ\"] = df.sum()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£è¦åŒ–å‰: ï¼¡ï¼¢ï¼£ABCï½ï½‚ï½ƒabcï½±ï½²ï½³ã‚¢ã‚¤ã‚¦â‘ â‘¡â‘¢123\n",
      "æ­£è¦åŒ–å¾Œ: ABCABCabcabcã‚¢ã‚¤ã‚¦ã‚¢ã‚¤ã‚¦123123\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "# ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦Unicodeæ­£è¦åŒ–ã‚’è¡Œã†\n",
    "text = \"ï¼¡ï¼¢ï¼£ABCï½ï½‚ï½ƒabcï½±ï½²ï½³ã‚¢ã‚¤ã‚¦â‘ â‘¡â‘¢123\"\n",
    "normalized_text = normalize(\"NFKC\", text)\n",
    "print(f\"æ­£è¦åŒ–å‰: {text}\")\n",
    "print(f\"æ­£è¦åŒ–å¾Œ: {normalized_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£è¦åŒ–å‰: ãˆ±ã€3ãŒ•ã€10â„ƒ\n",
      "æ­£è¦åŒ–å¾Œ: (æ ª)ã€3ã‚­ãƒ­ã‚°ãƒ©ãƒ ã€10Â°C\n"
     ]
    }
   ],
   "source": [
    "text = \"ãˆ±ã€3ãŒ•ã€10â„ƒ\"\n",
    "normalized_text = normalize(\"NFKC\", text)\n",
    "print(f\"æ­£è¦åŒ–å‰: {text}\")\n",
    "print(f\"æ­£è¦åŒ–å¾Œ: {normalized_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰å˜ä½: ã•ãã‚‰/å­¦é™¢/ã€/C/##ia/##o/Sm/##ile/##s/ã®/ãƒ¡ãƒ³ãƒãƒ¼/ã€‚\n",
      "æ–‡å­—å˜ä½: ã•/ã/ã‚‰/å­¦/é™¢/ã€/C/i/a/o/ /S/m/i/l/e/s/ã®/ãƒ¡/ãƒ³/ãƒ/ãƒ¼/ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’èª­ã¿è¾¼ã‚€\n",
    "model_name = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†\n",
    "subwords = \"/\".join(tokenizer.tokenize(dataset[\"train\"][0][\"text\"]))\n",
    "characters = \"/\".join(dataset[\"train\"][0][\"text\"])\n",
    "print(f\"ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰å˜ä½: {subwords}\")\n",
    "print(f\"æ–‡å­—å˜ä½: {characters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡å­—ã®list: ['ã•', 'ã', 'ã‚‰', 'å­¦', 'é™¢']\n",
      "ãƒˆãƒ¼ã‚¯ãƒ³ã®list: ['[CLS]', 'ã•ãã‚‰', 'å­¦é™¢', '[SEP]']\n",
      "æ–‡å­—ã«å¯¾ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®: [[1], [1], [1], [2], [2]]\n",
      "ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã™ã‚‹æ–‡å­—ã®ä½ç½®: [[], [0, 1, 2], [3, 4], []]\n"
     ]
    }
   ],
   "source": [
    "from spacy_alignments import get_alignments\n",
    "text = \"ã•ãã‚‰å­¦é™¢\"\n",
    "characters = list(text)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text))\n",
    "\n",
    "char_to_token_indices, token_to_char_indices = get_alignments(characters, tokens)\n",
    "print(f\"æ–‡å­—ã®list: {characters}\")\n",
    "print(f\"ãƒˆãƒ¼ã‚¯ãƒ³ã®list: {tokens}\")\n",
    "print(f\"æ–‡å­—ã«å¯¾ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®: {char_to_token_indices}\")\n",
    "print(f\"ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã™ã‚‹æ–‡å­—ã®ä½ç½®: {token_to_char_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"å¤§è°·ç¿”å¹³ã¯å²©æ‰‹çœŒæ°´æ²¢å¸‚å‡ºèº«\"\n",
    "entities = [\n",
    "    {\"name\": \"å¤§è°·ç¿”å¹³\", \"span\": [0, 4], \"type\": \"äººå\"},\n",
    "    {\"name\": \"å²©æ‰‹çœŒæ°´æ²¢å¸‚\", \"span\": [5, 11], \"type\": \"åœ°å\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ä½ç½®</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ãƒˆãƒ¼ã‚¯ãƒ³åˆ—</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>å¤§è°·</td>\n",
       "      <td>ç¿”</td>\n",
       "      <td>##å¹³</td>\n",
       "      <td>ã¯</td>\n",
       "      <td>å²©æ‰‹</td>\n",
       "      <td>çœŒ</td>\n",
       "      <td>æ°´æ²¢</td>\n",
       "      <td>å¸‚</td>\n",
       "      <td>å‡ºèº«</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ãƒ©ãƒ™ãƒ«åˆ—</th>\n",
       "      <td>-</td>\n",
       "      <td>B-äººå</td>\n",
       "      <td>I-äººå</td>\n",
       "      <td>I-äººå</td>\n",
       "      <td>O</td>\n",
       "      <td>B-åœ°å</td>\n",
       "      <td>I-åœ°å</td>\n",
       "      <td>I-åœ°å</td>\n",
       "      <td>I-åœ°å</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ä½ç½®        0     1     2     3  4     5     6     7     8   9      10\n",
       "ãƒˆãƒ¼ã‚¯ãƒ³åˆ—  [CLS]    å¤§è°·     ç¿”   ##å¹³  ã¯    å²©æ‰‹     çœŒ    æ°´æ²¢     å¸‚  å‡ºèº«  [SEP]\n",
       "ãƒ©ãƒ™ãƒ«åˆ—       -  B-äººå  I-äººå  I-äººå  O  B-åœ°å  I-åœ°å  I-åœ°å  I-åœ°å   O      -"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "def output_tokens_and_labels(\n",
    "    text: str,\n",
    "    entities: list[dict[str, list[int] | str]],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    ") -> tuple[list[str], list[str]]:\n",
    "    \"\"\"ãƒˆãƒ¼ã‚¯ãƒ³ã®listã¨ãƒ©ãƒ™ãƒ«ã®listã‚’å‡ºåŠ›\"\"\"\n",
    "    # æ–‡å­—ã®listã¨ãƒˆãƒ¼ã‚¯ãƒ³ã®listã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’ã¨ã‚‹\n",
    "    characters = list(text)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text))\n",
    "    char_to_token_indices, _ = get_alignments(characters, tokens)\n",
    "\n",
    "    # \"O\"ã®ãƒ©ãƒ™ãƒ«ã§åˆæœŸåŒ–ã—ãŸãƒ©ãƒ™ãƒ«ã®listã‚’ä½œæˆã™ã‚‹\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "    for entity in entities: # å„å›ºæœ‰è¡¨ç¾ã§å‡¦ç†ã™ã‚‹\n",
    "        entity_span, entity_type = entity[\"span\"], entity[\"type\"]\n",
    "        start = char_to_token_indices[entity_span[0]][0]\n",
    "        end = char_to_token_indices[entity_span[1] - 1][0]\n",
    "        # å›ºæœ‰è¡¨ç¾ã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã«\"B-\"ã®ãƒ©ãƒ™ãƒ«ã‚’è¨­å®šã™ã‚‹\n",
    "        labels[start] = f\"B-{entity_type}\"\n",
    "        # å›ºæœ‰è¡¨ç¾ã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ä»¥å¤–ã®ä½ç½®ã«\"I-\"ã®ãƒ©ãƒ™ãƒ«ã‚’è¨­å®šã™ã‚‹\n",
    "        for idx in range(start + 1, end + 1):\n",
    "            labels[idx] = f\"I-{entity_type}\"\n",
    "    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã«ã¯ãƒ©ãƒ™ãƒ«ã‚’è¨­å®šã—ãªã„\n",
    "    labels[0] = \"-\"\n",
    "    labels[-1] = \"-\"\n",
    "    return tokens, labels\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒ³ã¨ãƒ©ãƒ™ãƒ«ã®listã‚’å‡ºåŠ›ã™ã‚‹\n",
    "tokens, labels = output_tokens_and_labels(text, entities, tokenizer)\n",
    "# DataFrameã®å½¢å¼ã§è¡¨ç¤ºã™ã‚‹\n",
    "df = pd.DataFrame({\"ãƒˆãƒ¼ã‚¯ãƒ³åˆ—\": tokens, \"ãƒ©ãƒ™ãƒ«åˆ—\": labels})\n",
    "df.index.name = \"ä½ç½®\"\n",
    "display(df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "def create_character_labels(\n",
    "    text: str, entities: list[dict[str, list[int] | str]]\n",
    ") -> list[str]:\n",
    "    \"\"\"æ–‡å­—ãƒ™ãƒ¼ã‚¹ã§ãƒ©ãƒ™ãƒ«ã®listã‚’ä½œæˆ\"\"\"\n",
    "    # \"O\"ã®ãƒ©ãƒ™ãƒ«ã§åˆæœŸåŒ–ã—ãŸãƒ©ãƒ™ãƒ«ã®listã‚’ä½œæˆã™ã‚‹\n",
    "    labels = [\"O\"] * len(text)\n",
    "    for entity in entities: # å„å›ºæœ‰è¡¨ç¾ã‚’å‡¦ç†ã™ã‚‹\n",
    "        entity_span, entity_type = entity[\"span\"], entity[\"type\"]\n",
    "        # å›ºæœ‰è¡¨ç¾ã®é–‹å§‹æ–‡å­—ã®ä½ç½®ã«\"B-\"ã®ãƒ©ãƒ™ãƒ«ã‚’è¨­å®šã™ã‚‹\n",
    "        labels[entity_span[0]] = f\"B-{entity_type}\"\n",
    "        # å›ºæœ‰è¡¨ç¾ã®é–‹å§‹æ–‡å­—ä»¥å¤–ã®ä½ç½®ã«\"I-\"ã®ãƒ©ãƒ™ãƒ«ã‚’è¨­å®šã™ã‚‹\n",
    "        for i in range(entity_span[0] + 1, entity_span[1]):\n",
    "            labels[i] = f\"I-{entity_type}\"\n",
    "    return labels\n",
    "\n",
    "def convert_results_to_labels(\n",
    "    results: list[dict[str, Any]]\n",
    ") -> tuple[list[list[str]], list[list[str]]]:\n",
    "    \"\"\"æ­£è§£ãƒ‡ãƒ¼ã‚¿ã¨äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«ã®listã‚’ä½œæˆ\"\"\"\n",
    "    true_labels, pred_labels = [], []\n",
    "    for result in results: # å„äº‹ä¾‹ã‚’å‡¦ç†ã™ã‚‹\n",
    "        # æ–‡å­—ãƒ™ãƒ¼ã‚¹ã§ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¦listã«åŠ ãˆã‚‹\n",
    "        true_labels.append(\n",
    "            create_character_labels(result[\"text\"], result[\"entities\"])\n",
    "        )\n",
    "        pred_labels.append(\n",
    "            create_character_labels(result[\"text\"], result[\"pred_entities\"])\n",
    "        )\n",
    "    return true_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_scores(\n",
    "    true_labels: list[list[str]], pred_labels: list[list[str]], average: str\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"é©åˆç‡ã€å†ç¾ç‡ã€Få€¤ã‚’ç®—å‡º\"\"\"\n",
    "    scores = {\n",
    "        \"precision\": precision_score(true_labels, pred_labels, average=average),\n",
    "        \"recall\": recall_score(true_labels, pred_labels, average=average),\n",
    "        \"f1-score\": f1_score(true_labels, pred_labels, average=average),\n",
    "    }\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O',\n",
      " 1: 'B-ãã®ä»–ã®çµ„ç¹”å',\n",
      " 2: 'I-ãã®ä»–ã®çµ„ç¹”å',\n",
      " 3: 'B-ã‚¤ãƒ™ãƒ³ãƒˆå',\n",
      " 4: 'I-ã‚¤ãƒ™ãƒ³ãƒˆå',\n",
      " 5: 'B-äººå',\n",
      " 6: 'I-äººå',\n",
      " 7: 'B-åœ°å',\n",
      " 8: 'I-åœ°å',\n",
      " 9: 'B-æ”¿æ²»çš„çµ„ç¹”å',\n",
      " 10: 'I-æ”¿æ²»çš„çµ„ç¹”å',\n",
      " 11: 'B-æ–½è¨­å',\n",
      " 12: 'I-æ–½è¨­å',\n",
      " 13: 'B-æ³•äººå',\n",
      " 14: 'I-æ³•äººå',\n",
      " 15: 'B-è£½å“å',\n",
      " 16: 'I-è£½å“å'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def create_label2id(\n",
    "    entities_list: list[list[dict[str, str | int]]]\n",
    ") -> dict[str, int]:\n",
    "    \"\"\"ãƒ©ãƒ™ãƒ«ã¨IDã‚’ç´ä»˜ã‘ã‚‹dictã‚’ä½œæˆ\"\"\"\n",
    "    # \"O\"ã®IDã«ã¯0ã‚’å‰²ã‚Šå½“ã¦ã‚‹\n",
    "    label2id = {\"O\": 0}\n",
    "    # å›ºæœ‰è¡¨ç¾ã‚¿ã‚¤ãƒ—ã®setã‚’ç²å¾—ã—ã¦ä¸¦ã³æ›¿ãˆã‚‹\n",
    "    entity_types = set(\n",
    "        [e[\"type\"] for entities in entities_list for e in entities]\n",
    "    )\n",
    "    entity_types = sorted(entity_types)\n",
    "    for i, entity_type in enumerate(entity_types):\n",
    "        # \"B-\"ã®IDã«ã¯å¥‡æ•°ç•ªå·ã‚’å‰²ã‚Šå½“ã¦ã‚‹\n",
    "        label2id[f\"B-{entity_type}\"] = i * 2 + 1\n",
    "        # \"I-\"ã®IDã«ã¯å¶æ•°ç•ªå·ã‚’å‰²ã‚Šå½“ã¦ã‚‹\n",
    "        label2id[f\"I-{entity_type}\"] = i * 2 + 2\n",
    "    return label2id\n",
    "\n",
    "# ãƒ©ãƒ™ãƒ«ã¨IDã‚’ç´ä»˜ã‘ã‚‹dictã‚’ä½œæˆã™ã‚‹\n",
    "label2id = create_label2id(dataset[\"train\"][\"entities\"])\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "pprint(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'fn_kwargs'={'tokenizer': BertJapaneseTokenizer(name_or_path='cl-tohoku/bert-base-japanese-v3', vocab_size=32768, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}, 'label2id': {'O': 0, 'B-ãã®ä»–ã®çµ„ç¹”å': 1, 'I-ãã®ä»–ã®çµ„ç¹”å': 2, 'B-ã‚¤ãƒ™ãƒ³ãƒˆå': 3, 'I-ã‚¤ãƒ™ãƒ³ãƒˆå': 4, 'B-äººå': 5, 'I-äººå': 6, 'B-åœ°å': 7, 'I-åœ°å': 8, 'B-æ”¿æ²»çš„çµ„ç¹”å': 9, 'I-æ”¿æ²»çš„çµ„ç¹”å': 10, 'B-æ–½è¨­å': 11, 'I-æ–½è¨­å': 12, 'B-æ³•äººå': 13, 'I-æ³•äººå': 14, 'B-è£½å“å': 15, 'I-è£½å“å': 16}} of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1ab93522f34d7b8eac83841a631c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4274 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c767474ede9442bf88a14205ef1d3ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/534 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "def preprocess_data(\n",
    "    data: dict[str, Any],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    label2id: dict[int, str],\n",
    ") -> BatchEncoding:\n",
    "    \"\"\"ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\"\"\"\n",
    "    # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†\n",
    "    inputs = tokenizer(\n",
    "        data[\"text\"],\n",
    "        return_tensors=\"pt\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "\n",
    "    # æ–‡å­—ã®listã¨ãƒˆãƒ¼ã‚¯ãƒ³ã®listã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’ã¨ã‚‹\n",
    "    characters = list(data[\"text\"])\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n",
    "    char_to_token_indices, _ = get_alignments(characters, tokens)\n",
    "\n",
    "    # \"O\"ã®IDã®listã‚’ä½œæˆã™ã‚‹\n",
    "    labels = torch.zeros_like(inputs[\"input_ids\"])\n",
    "    for entity in data[\"entities\"]: # å„å›ºæœ‰è¡¨ç¾ã‚’å‡¦ç†ã™ã‚‹\n",
    "        start_token_indices = char_to_token_indices[entity[\"span\"][0]]\n",
    "        end_token_indices = char_to_token_indices[\n",
    "            entity[\"span\"][1] - 1\n",
    "        ]\n",
    "        # æ–‡å­—ã«å¯¾å¿œã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãŒå­˜åœ¨ã—ãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹\n",
    "        if (\n",
    "            len(start_token_indices) == 0\n",
    "            or len(end_token_indices) == 0\n",
    "        ):\n",
    "            continue\n",
    "        start, end = start_token_indices[0], end_token_indices[0]\n",
    "        entity_type = entity[\"type\"]\n",
    "        # å›ºæœ‰è¡¨ç¾ã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã«\"B-\"ã®IDã‚’è¨­å®šã™ã‚‹\n",
    "        labels[start] = label2id[f\"B-{entity_type}\"]\n",
    "        # å›ºæœ‰è¡¨ç¾ã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ä»¥å¤–ã®ä½ç½®ã«\"I-\"ã®IDã‚’è¨­å®šã™ã‚‹\n",
    "        if start != end:\n",
    "            labels[start + 1 : end + 1] = label2id[f\"I-{entity_type}\"]\n",
    "    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã®IDã¯-100ã¨ã™ã‚‹\n",
    "    labels[torch.where(inputs[\"special_tokens_mask\"])] = -100\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n",
    "\n",
    "# è¨“ç·´ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦å‰å‡¦ç†ã‚’è¡Œã†\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    preprocess_data,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"label2id\": label2id,\n",
    "    },\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "# æ¤œè¨¼ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦å‰å‡¦ç†ã‚’è¡Œã†\n",
    "validation_dataset = dataset[\"validation\"].map(\n",
    "    preprocess_data,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"label2id\": label2id,\n",
    "    },\n",
    "    remove_columns=dataset[\"validation\"].column_names,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, label2id=label2id, id2label=id2label\n",
    ")\n",
    "# collateé–¢æ•°ã«DataCollatorForTokenClassificationã‚’ç”¨ã„ã‚‹\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/training_args.py:1540: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ğŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c751072003446b8b936778c1daeb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/670 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "# ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’42ã«å›ºå®šã™ã‚‹\n",
    "set_seed(42)\n",
    "\n",
    "# Trainerã«æ¸¡ã™å¼•æ•°ã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_bert_ner\", # çµæœã®ä¿å­˜ãƒ•ã‚©ãƒ«ãƒ€\n",
    "    per_device_train_batch_size=32, # è¨“ç·´æ™‚ã®ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    per_device_eval_batch_size=32, # è©•ä¾¡æ™‚ã®ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    learning_rate=1e-4, # å­¦ç¿’ç‡\n",
    "    lr_scheduler_type=\"linear\", # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©\n",
    "    warmup_ratio=0.1, # å­¦ç¿’ç‡ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—\n",
    "    num_train_epochs=5, # è¨“ç·´ã‚¨ãƒãƒƒã‚¯æ•°\n",
    "    evaluation_strategy=\"epoch\", # è©•ä¾¡ã‚¿ã‚¤ãƒŸãƒ³ã‚°\n",
    "    save_strategy=\"epoch\", # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜ã‚¿ã‚¤ãƒŸãƒ³ã‚°\n",
    "    logging_strategy=\"epoch\", # ãƒ­ã‚®ãƒ³ã‚°ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°\n",
    "    fp16=False, # è‡ªå‹•æ··åˆç²¾åº¦æ¼”ç®—ã®æœ‰åŠ¹åŒ–\n",
    "    no_cuda=True,\n",
    ")\n",
    "\n",
    "# Trainerã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# è¨“ç·´ã™ã‚‹\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
