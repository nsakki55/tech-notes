{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231f04e6ce3d42768815a1300918aefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/641k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce6d9b01d4e4e1286a9603f4b82671c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191bec7f343f43d9846b91210612a60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c32f4d66d0f4424b9f21ce84ac9e8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# データセットを読み込む\n",
    "dataset = load_dataset(\"llm-book/ner-wikipedia-dataset\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 4274\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 534\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 535\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'curid': '3638038',\n",
      "  'entities': [{'name': 'さくら学院', 'span': [0, 5], 'type': 'その他の組織名'},\n",
      "               {'name': 'Ciao Smiles', 'span': [6, 17], 'type': 'その他の組織名'}],\n",
      "  'text': 'さくら学院、Ciao Smilesのメンバー。'},\n",
      " {'curid': '1729527',\n",
      "  'entities': [{'name': 'レクレアティーボ・ウェルバ', 'span': [17, 30], 'type': 'その他の組織名'},\n",
      "               {'name': 'プリメーラ・ディビシオン', 'span': [32, 44], 'type': 'その他の組織名'}],\n",
      "  'text': '2008年10月5日、アウェーでのレクレアティーボ・ウェルバ戦でプリメーラ・ディビシオンでの初得点を決めた。'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(list(dataset[\"train\"])[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>人名</th>\n",
       "      <td>2394</td>\n",
       "      <td>299</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>法人名</th>\n",
       "      <td>2006</td>\n",
       "      <td>231</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>地名</th>\n",
       "      <td>1769</td>\n",
       "      <td>184</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>政治的組織名</th>\n",
       "      <td>953</td>\n",
       "      <td>121</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>製品名</th>\n",
       "      <td>934</td>\n",
       "      <td>123</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>施設名</th>\n",
       "      <td>868</td>\n",
       "      <td>103</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>その他の組織名</th>\n",
       "      <td>852</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>イベント名</th>\n",
       "      <td>831</td>\n",
       "      <td>85</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>合計</th>\n",
       "      <td>10607</td>\n",
       "      <td>1245</td>\n",
       "      <td>1333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train  validation  test\n",
       "人名        2394         299   287\n",
       "法人名       2006         231   248\n",
       "地名        1769         184   204\n",
       "政治的組織名     953         121   106\n",
       "製品名        934         123   158\n",
       "施設名        868         103   137\n",
       "その他の組織名    852          99   100\n",
       "イベント名      831          85    93\n",
       "合計       10607        1245  1333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def count_label_occurrences(dataset: Dataset) -> dict[str, int]:\n",
    "    \"\"\"固有表現タイプの出現回数をカウント\"\"\"\n",
    "    # 各事例から固有表現タイプを抽出したlistを作成する\n",
    "    entities = [\n",
    "        e[\"type\"] for data in dataset for e in data[\"entities\"]\n",
    "    ]\n",
    "\n",
    "    # ラベルの出現回数が多い順に並び替える\n",
    "    label_counts = dict(Counter(entities).most_common())\n",
    "    return label_counts\n",
    "\n",
    "label_counts_dict = {}\n",
    "for split in dataset: # 各分割セットを処理する\n",
    "    label_counts_dict[split] = count_label_occurrences(dataset[split])\n",
    "# DataFrame形式で表示する\n",
    "df = pd.DataFrame(label_counts_dict)\n",
    "df.loc[\"合計\"] = df.sum()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正規化前: ＡＢＣABCａｂｃabcｱｲｳアイウ①②③123\n",
      "正規化後: ABCABCabcabcアイウアイウ123123\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "# テキストに対してUnicode正規化を行う\n",
    "text = \"ＡＢＣABCａｂｃabcｱｲｳアイウ①②③123\"\n",
    "normalized_text = normalize(\"NFKC\", text)\n",
    "print(f\"正規化前: {text}\")\n",
    "print(f\"正規化後: {normalized_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正規化前: ㈱、3㌕、10℃\n",
      "正規化後: (株)、3キログラム、10°C\n"
     ]
    }
   ],
   "source": [
    "text = \"㈱、3㌕、10℃\"\n",
    "normalized_text = normalize(\"NFKC\", text)\n",
    "print(f\"正規化前: {text}\")\n",
    "print(f\"正規化後: {normalized_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サブワード単位: さくら/学院/、/C/##ia/##o/Sm/##ile/##s/の/メンバー/。\n",
      "文字単位: さ/く/ら/学/院/、/C/i/a/o/ /S/m/i/l/e/s/の/メ/ン/バ/ー/。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# トークナイザを読み込む\n",
    "model_name = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# トークナイゼーションを行う\n",
    "subwords = \"/\".join(tokenizer.tokenize(dataset[\"train\"][0][\"text\"]))\n",
    "characters = \"/\".join(dataset[\"train\"][0][\"text\"])\n",
    "print(f\"サブワード単位: {subwords}\")\n",
    "print(f\"文字単位: {characters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文字のlist: ['さ', 'く', 'ら', '学', '院']\n",
      "トークンのlist: ['[CLS]', 'さくら', '学院', '[SEP]']\n",
      "文字に対するトークンの位置: [[1], [1], [1], [2], [2]]\n",
      "トークンに対する文字の位置: [[], [0, 1, 2], [3, 4], []]\n"
     ]
    }
   ],
   "source": [
    "from spacy_alignments import get_alignments\n",
    "text = \"さくら学院\"\n",
    "characters = list(text)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text))\n",
    "\n",
    "char_to_token_indices, token_to_char_indices = get_alignments(characters, tokens)\n",
    "print(f\"文字のlist: {characters}\")\n",
    "print(f\"トークンのlist: {tokens}\")\n",
    "print(f\"文字に対するトークンの位置: {char_to_token_indices}\")\n",
    "print(f\"トークンに対する文字の位置: {token_to_char_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"大谷翔平は岩手県水沢市出身\"\n",
    "entities = [\n",
    "    {\"name\": \"大谷翔平\", \"span\": [0, 4], \"type\": \"人名\"},\n",
    "    {\"name\": \"岩手県水沢市\", \"span\": [5, 11], \"type\": \"地名\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>位置</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>トークン列</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>大谷</td>\n",
       "      <td>翔</td>\n",
       "      <td>##平</td>\n",
       "      <td>は</td>\n",
       "      <td>岩手</td>\n",
       "      <td>県</td>\n",
       "      <td>水沢</td>\n",
       "      <td>市</td>\n",
       "      <td>出身</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ラベル列</th>\n",
       "      <td>-</td>\n",
       "      <td>B-人名</td>\n",
       "      <td>I-人名</td>\n",
       "      <td>I-人名</td>\n",
       "      <td>O</td>\n",
       "      <td>B-地名</td>\n",
       "      <td>I-地名</td>\n",
       "      <td>I-地名</td>\n",
       "      <td>I-地名</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "位置        0     1     2     3  4     5     6     7     8   9      10\n",
       "トークン列  [CLS]    大谷     翔   ##平  は    岩手     県    水沢     市  出身  [SEP]\n",
       "ラベル列       -  B-人名  I-人名  I-人名  O  B-地名  I-地名  I-地名  I-地名   O      -"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "def output_tokens_and_labels(\n",
    "    text: str,\n",
    "    entities: list[dict[str, list[int] | str]],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    ") -> tuple[list[str], list[str]]:\n",
    "    \"\"\"トークンのlistとラベルのlistを出力\"\"\"\n",
    "    # 文字のlistとトークンのlistのアライメントをとる\n",
    "    characters = list(text)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text))\n",
    "    char_to_token_indices, _ = get_alignments(characters, tokens)\n",
    "\n",
    "    # \"O\"のラベルで初期化したラベルのlistを作成する\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "    for entity in entities: # 各固有表現で処理する\n",
    "        entity_span, entity_type = entity[\"span\"], entity[\"type\"]\n",
    "        start = char_to_token_indices[entity_span[0]][0]\n",
    "        end = char_to_token_indices[entity_span[1] - 1][0]\n",
    "        # 固有表現の開始トークンの位置に\"B-\"のラベルを設定する\n",
    "        labels[start] = f\"B-{entity_type}\"\n",
    "        # 固有表現の開始トークン以外の位置に\"I-\"のラベルを設定する\n",
    "        for idx in range(start + 1, end + 1):\n",
    "            labels[idx] = f\"I-{entity_type}\"\n",
    "    # 特殊トークンの位置にはラベルを設定しない\n",
    "    labels[0] = \"-\"\n",
    "    labels[-1] = \"-\"\n",
    "    return tokens, labels\n",
    "\n",
    "# トークンとラベルのlistを出力する\n",
    "tokens, labels = output_tokens_and_labels(text, entities, tokenizer)\n",
    "# DataFrameの形式で表示する\n",
    "df = pd.DataFrame({\"トークン列\": tokens, \"ラベル列\": labels})\n",
    "df.index.name = \"位置\"\n",
    "display(df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "def create_character_labels(\n",
    "    text: str, entities: list[dict[str, list[int] | str]]\n",
    ") -> list[str]:\n",
    "    \"\"\"文字ベースでラベルのlistを作成\"\"\"\n",
    "    # \"O\"のラベルで初期化したラベルのlistを作成する\n",
    "    labels = [\"O\"] * len(text)\n",
    "    for entity in entities: # 各固有表現を処理する\n",
    "        entity_span, entity_type = entity[\"span\"], entity[\"type\"]\n",
    "        # 固有表現の開始文字の位置に\"B-\"のラベルを設定する\n",
    "        labels[entity_span[0]] = f\"B-{entity_type}\"\n",
    "        # 固有表現の開始文字以外の位置に\"I-\"のラベルを設定する\n",
    "        for i in range(entity_span[0] + 1, entity_span[1]):\n",
    "            labels[i] = f\"I-{entity_type}\"\n",
    "    return labels\n",
    "\n",
    "def convert_results_to_labels(\n",
    "    results: list[dict[str, Any]]\n",
    ") -> tuple[list[list[str]], list[list[str]]]:\n",
    "    \"\"\"正解データと予測データのラベルのlistを作成\"\"\"\n",
    "    true_labels, pred_labels = [], []\n",
    "    for result in results: # 各事例を処理する\n",
    "        # 文字ベースでラベルのリストを作成してlistに加える\n",
    "        true_labels.append(\n",
    "            create_character_labels(result[\"text\"], result[\"entities\"])\n",
    "        )\n",
    "        pred_labels.append(\n",
    "            create_character_labels(result[\"text\"], result[\"pred_entities\"])\n",
    "        )\n",
    "    return true_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_scores(\n",
    "    true_labels: list[list[str]], pred_labels: list[list[str]], average: str\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"適合率、再現率、F値を算出\"\"\"\n",
    "    scores = {\n",
    "        \"precision\": precision_score(true_labels, pred_labels, average=average),\n",
    "        \"recall\": recall_score(true_labels, pred_labels, average=average),\n",
    "        \"f1-score\": f1_score(true_labels, pred_labels, average=average),\n",
    "    }\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O',\n",
      " 1: 'B-その他の組織名',\n",
      " 2: 'I-その他の組織名',\n",
      " 3: 'B-イベント名',\n",
      " 4: 'I-イベント名',\n",
      " 5: 'B-人名',\n",
      " 6: 'I-人名',\n",
      " 7: 'B-地名',\n",
      " 8: 'I-地名',\n",
      " 9: 'B-政治的組織名',\n",
      " 10: 'I-政治的組織名',\n",
      " 11: 'B-施設名',\n",
      " 12: 'I-施設名',\n",
      " 13: 'B-法人名',\n",
      " 14: 'I-法人名',\n",
      " 15: 'B-製品名',\n",
      " 16: 'I-製品名'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def create_label2id(\n",
    "    entities_list: list[list[dict[str, str | int]]]\n",
    ") -> dict[str, int]:\n",
    "    \"\"\"ラベルとIDを紐付けるdictを作成\"\"\"\n",
    "    # \"O\"のIDには0を割り当てる\n",
    "    label2id = {\"O\": 0}\n",
    "    # 固有表現タイプのsetを獲得して並び替える\n",
    "    entity_types = set(\n",
    "        [e[\"type\"] for entities in entities_list for e in entities]\n",
    "    )\n",
    "    entity_types = sorted(entity_types)\n",
    "    for i, entity_type in enumerate(entity_types):\n",
    "        # \"B-\"のIDには奇数番号を割り当てる\n",
    "        label2id[f\"B-{entity_type}\"] = i * 2 + 1\n",
    "        # \"I-\"のIDには偶数番号を割り当てる\n",
    "        label2id[f\"I-{entity_type}\"] = i * 2 + 2\n",
    "    return label2id\n",
    "\n",
    "# ラベルとIDを紐付けるdictを作成する\n",
    "label2id = create_label2id(dataset[\"train\"][\"entities\"])\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "pprint(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'fn_kwargs'={'tokenizer': BertJapaneseTokenizer(name_or_path='cl-tohoku/bert-base-japanese-v3', vocab_size=32768, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}, 'label2id': {'O': 0, 'B-その他の組織名': 1, 'I-その他の組織名': 2, 'B-イベント名': 3, 'I-イベント名': 4, 'B-人名': 5, 'I-人名': 6, 'B-地名': 7, 'I-地名': 8, 'B-政治的組織名': 9, 'I-政治的組織名': 10, 'B-施設名': 11, 'I-施設名': 12, 'B-法人名': 13, 'I-法人名': 14, 'B-製品名': 15, 'I-製品名': 16}} of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1ab93522f34d7b8eac83841a631c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4274 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c767474ede9442bf88a14205ef1d3ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/534 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "def preprocess_data(\n",
    "    data: dict[str, Any],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    label2id: dict[int, str],\n",
    ") -> BatchEncoding:\n",
    "    \"\"\"データの前処理\"\"\"\n",
    "    # テキストのトークナイゼーションを行う\n",
    "    inputs = tokenizer(\n",
    "        data[\"text\"],\n",
    "        return_tensors=\"pt\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "\n",
    "    # 文字のlistとトークンのlistのアライメントをとる\n",
    "    characters = list(data[\"text\"])\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n",
    "    char_to_token_indices, _ = get_alignments(characters, tokens)\n",
    "\n",
    "    # \"O\"のIDのlistを作成する\n",
    "    labels = torch.zeros_like(inputs[\"input_ids\"])\n",
    "    for entity in data[\"entities\"]: # 各固有表現を処理する\n",
    "        start_token_indices = char_to_token_indices[entity[\"span\"][0]]\n",
    "        end_token_indices = char_to_token_indices[\n",
    "            entity[\"span\"][1] - 1\n",
    "        ]\n",
    "        # 文字に対応するトークンが存在しなければスキップする\n",
    "        if (\n",
    "            len(start_token_indices) == 0\n",
    "            or len(end_token_indices) == 0\n",
    "        ):\n",
    "            continue\n",
    "        start, end = start_token_indices[0], end_token_indices[0]\n",
    "        entity_type = entity[\"type\"]\n",
    "        # 固有表現の開始トークンの位置に\"B-\"のIDを設定する\n",
    "        labels[start] = label2id[f\"B-{entity_type}\"]\n",
    "        # 固有表現の開始トークン以外の位置に\"I-\"のIDを設定する\n",
    "        if start != end:\n",
    "            labels[start + 1 : end + 1] = label2id[f\"I-{entity_type}\"]\n",
    "    # 特殊トークンの位置のIDは-100とする\n",
    "    labels[torch.where(inputs[\"special_tokens_mask\"])] = -100\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n",
    "\n",
    "# 訓練セットに対して前処理を行う\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    preprocess_data,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"label2id\": label2id,\n",
    "    },\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "# 検証セットに対して前処理を行う\n",
    "validation_dataset = dataset[\"validation\"].map(\n",
    "    preprocess_data,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"label2id\": label2id,\n",
    "    },\n",
    "    remove_columns=dataset[\"validation\"].column_names,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "\n",
    "# モデルを読み込む\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, label2id=label2id, id2label=id2label\n",
    ")\n",
    "# collate関数にDataCollatorForTokenClassificationを用いる\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/training_args.py:1540: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c751072003446b8b936778c1daeb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/670 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "# 乱数シードを42に固定する\n",
    "set_seed(42)\n",
    "\n",
    "# Trainerに渡す引数を初期化する\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_bert_ner\", # 結果の保存フォルダ\n",
    "    per_device_train_batch_size=32, # 訓練時のバッチサイズ\n",
    "    per_device_eval_batch_size=32, # 評価時のバッチサイズ\n",
    "    learning_rate=1e-4, # 学習率\n",
    "    lr_scheduler_type=\"linear\", # 学習率スケジューラ\n",
    "    warmup_ratio=0.1, # 学習率のウォームアップ\n",
    "    num_train_epochs=5, # 訓練エポック数\n",
    "    evaluation_strategy=\"epoch\", # 評価タイミング\n",
    "    save_strategy=\"epoch\", # チェックポイントの保存タイミング\n",
    "    logging_strategy=\"epoch\", # ロギングのタイミング\n",
    "    fp16=False, # 自動混合精度演算の有効化\n",
    "    no_cuda=True,\n",
    ")\n",
    "\n",
    "# Trainerを初期化する\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# 訓練する\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
