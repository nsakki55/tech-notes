{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.72M/1.72M [00:01<00:00, 884kB/s]\n",
      "Downloading data: 100%|██████████| 178k/178k [00:01<00:00, 131kB/s]\n",
      "Downloading data: 100%|██████████| 178k/178k [00:01<00:00, 127kB/s]\n",
      "Generating train split: 100%|██████████| 20149/20149 [00:00<00:00, 466824.08 examples/s]\n",
      "Generating validation split: 100%|██████████| 1608/1608 [00:00<00:00, 699993.86 examples/s]\n",
      "Generating test split: 100%|██████████| 1781/1781 [00:00<00:00, 1005661.74 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datetime': '2012/7/31 23:48',\n",
      " 'label': 1,\n",
      " 'sentence': 'ぼけっとしてたらこんな時間。チャリあるから食べにでたいのに…',\n",
      " 'user_id': 1}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"llm-book/wrime-sentiment\", split=\"train\")\n",
    "valid_dataset = load_dataset(\"llm-book/wrime-sentiment\", split=\"validation\")\n",
    "pprint(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datetime': Value(dtype='string', id=None),\n",
      " 'label': ClassLabel(names=['positive', 'negative'], id=None),\n",
      " 'sentence': Value(dtype='string', id=None),\n",
      " 'user_id': Value(dtype='int64', id=None)}\n"
     ]
    }
   ],
   "source": [
    "pprint(train_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /Users/satsuki/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/vocab.txt\n",
      "loading file spiece.model from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/satsuki/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertJapaneseTokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(type(tokenizer).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['これ', 'は', 'テスト', 'です', '。']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"これはテストです。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchEncoding\n"
     ]
    }
   ],
   "source": [
    "encoded_input =  tokenizer(\"これはテストです。\")\n",
    "print(type(encoded_input).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [2, 12538, 465, 14985, 13037, 385, 3],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "pprint(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'これ', 'は', 'テスト', 'です', '。', '[SEP]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20149/20149 [00:01<00:00, 13018.59it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1608/1608 [00:00<00:00, 9481.74it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def visualize_text_length(dataset: Dataset):\n",
    "    length_counter = Counter()\n",
    "    for data in tqdm(dataset):\n",
    "        length = len(tokenizer.tokenize(data[\"sentence\"]))\n",
    "        length_counter[length] += 1\n",
    "    plt.bar(length_counter.keys(), length_counter.values(), width=1.0)\n",
    "    plt.xlabel(\"token number\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_text_length(train_dataset)\n",
    "visualize_text_length(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datetime': '2020/5/15 17:35',\n",
      " 'label': 1,\n",
      " 'sentence': 'よくわからない連携。',\n",
      " 'user_id': 26}\n"
     ]
    }
   ],
   "source": [
    "for data in valid_dataset:\n",
    "    if len(tokenizer.tokenize(data[\"sentence\"])) < 10:\n",
    "        pprint(data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_lables(dataset: Dataset):\n",
    "    label_counter = Counter()\n",
    "    for data in dataset:\n",
    "        label_id = data[\"label\"]\n",
    "        label_name = dataset.features[\"label\"].names[label_id]\n",
    "        label_counter[label_name] += 1\n",
    "    \n",
    "    plt.bar(label_counter.keys(), label_counter.values(), width=1.0)\n",
    "    plt.xlabel(\"label name\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_lables(train_dataset) \n",
    "visualize_lables(valid_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BatchEncoding\n",
    "\n",
    "def preprocess_text_classification(example: dict[str, str | int]) -> BatchEncoding:\n",
    "    encoded_example = tokenizer(example[\"sentence\"], max_length=512)\n",
    "    encoded_example[\"labels\"] = example[\"label\"]\n",
    "\n",
    "    return encoded_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20149/20149 [00:01<00:00, 12736.74 examples/s]\n",
      "Map: 100%|██████████| 1608/1608 [00:00<00:00, 6830.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_train_dataset = train_dataset.map(preprocess_text_classification, remove_columns=train_dataset.column_names, batched=True)\n",
    "encoded_valid_dataset = valid_dataset.map(preprocess_text_classification, remove_columns=valid_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 478, 7139, 15269, 441, 456, 13123, 21311, 12671, 385, 12651, 7065, 12485, 12488, 13781, 461, 457, 13030, 464, 461, 29, 29, 29, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}\n"
     ]
    }
   ],
   "source": [
    "print(encoded_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs = data_collator(encoded_train_dataset[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,   478,  7139, 15269,   441,   456, 13123, 21311, 12671,   385,\n",
       "         12651,  7065, 12485, 12488, 13781,   461,   457, 13030,   464,   461,\n",
       "            29,    29,    29,     3,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [    2, 15028,   464,  2806,   484, 30908,   456, 19730,   385,  2724,\n",
       "          7998, 12505,  6483,   430, 14165,   456, 31265,  7106,   460,  7373,\n",
       "           458, 13513,  7241, 13320, 20831,  7951,   385, 12651,  7065, 14176,\n",
       "         13053,   484, 17941, 12494,   385,     3],\n",
       "        [    2,  4097,  7228,   384,  4097,  7203, 12494,   385,     3,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [    2, 19890,  4391,   430, 26297,   456,   431, 16328,   385, 15206,\n",
       "          7228,   460,    29,    29,    29,     3,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([1, 0, 1, 1])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/satsuki/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese-v3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"positive\",\n",
      "    \"1\": \"negative\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 1,\n",
      "    \"positive\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/satsuki/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/pytorch_model.bin\n",
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v3 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "class_label = train_dataset.features[\"label\"]\n",
    "label2id = {label: id for id, label in enumerate(class_label.names)}\n",
    "id2label = {id: label for id, label in enumerate(class_label.names)}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=class_label.num_classes,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.8075, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2869, -0.2644],\n",
       "        [ 0.1715,  0.0095],\n",
       "        [ 0.0040,  0.1524],\n",
       "        [ 0.2233, -0.2968]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(**data_collator(encoded_train_dataset[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/training_args.py:1540: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_wrime\",  # 結果の保存フォルダ\n",
    "    per_device_train_batch_size=4,  # 訓練時のバッチサイズ\n",
    "    per_device_eval_batch_size=4,  # 評価時のバッチサイズ\n",
    "    learning_rate=2e-5,  # 学習率\n",
    "    lr_scheduler_type=\"linear\",  # 学習率スケジューラの種類\n",
    "    warmup_ratio=0.1,  # 学習率のウォームアップの長さを指定\n",
    "    num_train_epochs=3,  # エポック数\n",
    "    save_strategy=\"epoch\",  # チェックポイントの保存タイミング\n",
    "    logging_strategy=\"epoch\",  # ロギングのタイミング\n",
    "    evaluation_strategy=\"epoch\",  # 検証セットによる評価のタイミング\n",
    "    load_best_model_at_end=True,  # 訓練後に開発セットで最良のモデルをロード\n",
    "    metric_for_best_model=\"accuracy\",  # 最良のモデルを決定する評価指標\n",
    "    fp16=False,  # 自動混合精度演算の有効化\n",
    "    no_cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def compute_accuracy(eval_pred: tuple[np.ndarray, np.ndarray])-> dict[str, float]:\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {\"accuracy\", (predictions==labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [01:12<?, ?it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 4\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3\n",
      "  Number of trainable parameters = 111,208,706\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      5\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mencoded_train_dataset[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m100\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/trainer.py:2236\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2233\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2236\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   2239\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/accelerate/data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[0;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=encoded_train_dataset[0:100],\n",
    "    eval_dataset=encoded_valid_dataset[:100],\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_accuracy,\n",
    "\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20149"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_train_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41fb65025722496b87dd5dba1d2628fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08545d80ce454f8fa91cbebe53501e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925eb03a32584abcab4b24725d4e83e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ff2c16441e42ffb5325034c5a9fd34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e360ad2136954000af9ef91ab4e273e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/231k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2e0235410d417fa7e021712f16a74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"llm-book/bert-base-japanese-v3-wrime-sentiment\"\n",
    "sentiment_pipeline = pipeline(model=model_name, device=\"cpu\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "valid_dataset = load_dataset(\n",
    "    \"llm-book/wrime-sentiment\", split=\"validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "161it [00:31,  5.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "class_label = valid_dataset.features[\"label\"]\n",
    "\n",
    "results: list[dict[str, float | str]] = []\n",
    "for i, example in tqdm(enumerate(valid_dataset.select([i for i in range(len(valid_dataset)) if i %10 == 0]))):\n",
    "    model_prediction = sentiment_pipeline(example[\"sentence\"])[0]\n",
    "    true_label = class_label.int2str(example[\"label\"])\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"example_id\": i,\n",
    "            \"pred_prod\": model_prediction[\"score\"],\n",
    "            \"pred_label\": model_prediction[\"label\"],\n",
    "            \"true_label\": true_label,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x194824390>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "confusion_matrix = confusion_matrix(\n",
    "    y_true=[result[\"true_label\"] for result in results],\n",
    "    y_pred=[result[\"pred_label\"] for result in results],\n",
    "    labels=class_label.names\n",
    ")\n",
    "\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix, display_labels=class_label.names\n",
    ").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "レビュー文：書けない\n",
      "予測：negative\n",
      "正解：positive\n",
      "----------------\n",
      "レビュー文：31のスーパーチョコミント食べたい でもどこもテナント入居してるビル自体が休業中でやってないのだけど。\n",
      "予測：negative\n",
      "正解：positive\n",
      "----------------\n",
      "レビュー文：テレビで3曲とも聞いた。音もこっちがいいかも。\n",
      "スマホと全然、すべてが違った。\n",
      "予測：negative\n",
      "正解：positive\n",
      "----------------\n",
      "レビュー文：本当に邪魔だからどっか行ってほしい。\n",
      "予測：positive\n",
      "正解：negative\n",
      "----------------\n",
      "レビュー文：ほんと、考察がすごい。\n",
      "自分だけではわからなかったこととか多すぎる。\n",
      "他のことにも援用できる。\n",
      "物の見方が広がる。\n",
      "予測：positive\n",
      "正解：negative\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "# 予測が誤った事例を収集\n",
    "failed_results = [\n",
    "    res for res in results if res[\"pred_label\"] != res[\"true_label\"]\n",
    "]\n",
    "# モデルの予測確率が高い順にソート\n",
    "sorted_failed_results = sorted(\n",
    "    failed_results, key=lambda x: -x[\"pred_prod\"]\n",
    ")\n",
    "# 高い確率で予測しながら誤った事例の上位2件を表示\n",
    "for top_result in sorted_failed_results[:5]:\n",
    "    review_text = valid_dataset[top_result[\"example_id\"]][\"sentence\"]\n",
    "    print(f\"レビュー文：{review_text}\")\n",
    "    print(f\"予測：{top_result['pred_label']}\")\n",
    "    print(f\"正解：{top_result['true_label']}\")\n",
    "    # print(f\"予測確率: {top_result['pred_prob']:.4f}\")\n",
    "    print(\"----------------\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import set_seed\n",
    "set_seed(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98023d502b04e51a7a8dc71a9aebfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/5.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8714c27f62645eb8663c5a856396eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/93.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821563b567f74c2d8493cff97b2cc78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e53a83ea1d431eab20f81f9baed7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 2,\n",
      " 'sentence1': '二人の男性がジャンボジェット機を見ています。',\n",
      " 'sentence2': '2人の男性が、白い飛行機を眺めています。',\n",
      " 'sentence_pair_id': '0',\n",
      " 'yjcaptions_id': '100124-104404-104405'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Hugging Face Hub上のllm-book/JGLUEのリポジトリから\n",
    "# JNLIのデータを読み込む\n",
    "train_dataset = load_dataset(\n",
    "    \"llm-book/JGLUE\", name=\"JNLI\", split=\"train\"\n",
    ")\n",
    "valid_dataset = load_dataset(\n",
    "    \"llm-book/JGLUE\", name=\"JNLI\", split=\"validation\"\n",
    ")\n",
    "# pprintで見やすく表示する\n",
    "pprint(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(names=['entailment', 'contradiction', 'neutral'], id=None)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.features[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /Users/satsuki/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/vocab.txt\n",
      "loading file spiece.model from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/satsuki/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at None\n",
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BatchEncoding\n",
    "\n",
    "def preprocess_text_pair_classification(example: dict[str, str|int])-> BatchEncoding:\n",
    "    encoded_example = tokenizer(\n",
    "        example[\"sentence1\"], example[\"sentence2\"], max_length=128\n",
    "    )\n",
    "    encoded_example[\"labels\"] = example[\"label\"]\n",
    "    return encoded_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_text_pair_classification at 0x1985f71a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d98bb39a9074b56a4f21053bc3b93b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20073 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339dba3c7449465fbba3d54481f9d61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2434 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_train_dataset = train_dataset.map(\n",
    "    preprocess_text_pair_classification,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "encoded_valid_dataset = valid_dataset.map(\n",
    "    preprocess_text_pair_classification,\n",
    "    remove_columns=valid_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 14269, 464, 13341, 430, 27914, 16753, 3107, 500, 5538, 456, 422, 12995, 385, 3, 33, 680, 464, 13341, 430, 384, 16517, 13208, 3107, 500, 29887, 456, 422, 12995, 385, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 2}\n"
     ]
    }
   ],
   "source": [
    "print(encoded_train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '二人',\n",
       " 'の',\n",
       " '男性',\n",
       " 'が',\n",
       " 'ジャンボ',\n",
       " 'ジェット',\n",
       " '機',\n",
       " 'を',\n",
       " '見',\n",
       " 'て',\n",
       " 'い',\n",
       " 'ます',\n",
       " '。',\n",
       " '[SEP]',\n",
       " '2',\n",
       " '人',\n",
       " 'の',\n",
       " '男性',\n",
       " 'が',\n",
       " '、',\n",
       " '白い',\n",
       " '飛行',\n",
       " '機',\n",
       " 'を',\n",
       " '眺め',\n",
       " 'て',\n",
       " 'い',\n",
       " 'ます',\n",
       " '。',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoded_train_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': torch.Size([4, 31]),\n",
      " 'input_ids': torch.Size([4, 31]),\n",
      " 'labels': torch.Size([4]),\n",
      " 'token_type_ids': torch.Size([4, 31])}\n"
     ]
    }
   ],
   "source": [
    "batch_inputs = data_collator(encoded_train_dataset[0:4])\n",
    "pprint({name: tensor.size() for name, tensor in batch_inputs.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/satsuki/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese-v3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"contradiction\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 1,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/satsuki/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/pytorch_model.bin\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v3 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "class_label = train_dataset.features[\"label\"]\n",
    "label2id = {label: id for id, label in enumerate(class_label.names)}\n",
    "id2label = {id: label for id, label in enumerate(class_label.names)}\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=class_label.num_classes,\n",
    "    label2id=label2id,  # ラベル名からIDへの対応を指定\n",
    "    id2label=id2label,  # IDからラベル名への対応を指定\n",
    ")\n",
    "print(type(model).__name__)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(1.2049, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1307,  0.1713, -0.0987],\n",
      "        [ 0.1554,  0.1772, -0.0990],\n",
      "        [ 0.0330,  0.0935, -0.0744],\n",
      "        [ 0.0430,  0.0790, -0.0806]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(model.forward(**data_collator(encoded_train_dataset[0:4])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/training_args.py:1540: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_jnli\",  # 結果の保存フォルダ\n",
    "    per_device_train_batch_size=32,  # 訓練時のバッチサイズ\n",
    "    per_device_eval_batch_size=32,  # 評価時のバッチサイズ\n",
    "    learning_rate=2e-5,  # 学習率\n",
    "    lr_scheduler_type=\"linear\",  # 学習率スケジューラの種類\n",
    "    warmup_ratio=0.1,  # 学習率のウォームアップの長さを指定\n",
    "    num_train_epochs=3,  # エポック数\n",
    "    save_strategy=\"epoch\",  # チェックポイントの保存タイミング\n",
    "    logging_strategy=\"epoch\",  # ロギングのタイミング\n",
    "    evaluation_strategy=\"epoch\",  # 検証セットによる評価のタイミング\n",
    "    load_best_model_at_end=True,  # 訓練後に開発セットで最良のモデルをロード\n",
    "    metric_for_best_model=\"accuracy\",  # 最良のモデルを決定する評価指標\n",
    "    fp16=False,  # 自動混合精度演算の有効化\n",
    "    no_cuda=True,\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_accuracy(\n",
    "    eval_pred: tuple[np.ndarray, np.ndarray]\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"予測ラベルと正解ラベルから正解率を計算\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # predictionsは各ラベルについてのスコア\n",
    "    # 最もスコアの高いインデックスを予測ラベルとする\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 20,073\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,884\n",
      "  Number of trainable parameters = 111,209,475\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5537c9719a2c48749eb91fceed3cf5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1884 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      5\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mencoded_train_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_accuracy,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:2134\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2134\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "# 乱数シードを42に固定\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datetime': '2012/7/31 23:48',\n",
      " 'label': 1,\n",
      " 'sentence': 'ぼけっとしてたらこんな時間。チャリあるから食べにでたいのに…',\n",
      " 'user_id': 1}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Hugging Face Hub上のllm-book/wrime-sentimentのリポジトリから\n",
    "# データを読み込む\n",
    "train_dataset = load_dataset(\"llm-book/wrime-sentiment\", split=\"train\")\n",
    "valid_dataset = load_dataset(\"llm-book/wrime-sentiment\", split=\"validation\")\n",
    "# pprintで見やすく表示する\n",
    "pprint(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertJapaneseTokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Hugging Face Hub上のモデル名を指定\n",
    "model_name = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "# モデル名からトークナイザを読み込む\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# トークナイザのクラス名を確認\n",
    "print(type(tokenizer).__name__)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "def preprocess_text_classification(\n",
    "    example: dict[str, str | int]\n",
    ") -> BatchEncoding:\n",
    "    \"\"\"文書分類の事例のテキストをトークナイズし、IDに変換\"\"\"\n",
    "    encoded_example = tokenizer(example[\"sentence\"], max_length=512)\n",
    "    # モデルの入力引数である\"labels\"をキーとして格納する\n",
    "    encoded_example[\"labels\"] = example[\"label\"]\n",
    "    return encoded_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_text_classification at 0x1487b9b20> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bb00f5a2494cf58f001ad2684e1038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20149 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c138948017ba49788fb9f6f257ad6325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1608 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "encoded_train_dataset = train_dataset.map(\n",
    "    preprocess_text_classification,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "encoded_valid_dataset = valid_dataset.map(\n",
    "    preprocess_text_classification,\n",
    "    remove_columns=valid_dataset.column_names,\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "class_label = train_dataset.features[\"label\"]\n",
    "label2id = {label: id for id, label in enumerate(class_label.names)}\n",
    "id2label = {id: label for id, label in enumerate(class_label.names)}\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=class_label.num_classes,\n",
    "    label2id=label2id,  # ラベル名からIDへの対応を指定\n",
    "    id2label=id2label,  # IDからラベル名への対応を指定\n",
    ")\n",
    "print(type(base_model).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification\n"
     ]
    }
   ],
   "source": [
    "import peft\n",
    "\n",
    "peft_config = peft.LoraConfig(\n",
    "    task_type=peft.TaskType.SEQ_CLS,  # モデルが解くタスクのタイプを指定\n",
    "    r=8,  # 差分行列のランク\n",
    "    lora_alpha=32,  #  LoRA層の出力のスケールを調節するハイパーパラメータ\n",
    "    lora_dropout=0.1,  # LoRA層に適用するドロップアウト\n",
    "    inference_mode=False,  # 推論モードの設定（今回は学習時なのでFalse）\n",
    ")\n",
    "model = peft.get_peft_model(base_model, peft_config)\n",
    "print(type(model).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 296,450 || all params: 111,505,156 || trainable%: 0.2659\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8075, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2869, -0.2644],\n",
      "        [ 0.1715,  0.0095],\n",
      "        [ 0.0040,  0.1524],\n",
      "        [ 0.2233, -0.2968]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(model.forward(**data_collator(encoded_train_dataset[0:4])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/training_args.py:1540: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_wrime_lora\",  # 結果の保存フォルダ\n",
    "    per_device_train_batch_size=32,  # 訓練時のバッチサイズ\n",
    "    per_device_eval_batch_size=32,  # 評価時のバッチサイズ\n",
    "    learning_rate=2e-4,  # 学習率\n",
    "    lr_scheduler_type=\"linear\",  # 学習率スケジューラの種類\n",
    "    warmup_ratio=0.1,  # 学習率のウォームアップの長さを指定\n",
    "    num_train_epochs=3,  # エポック数\n",
    "    save_strategy=\"epoch\",  # チェックポイントの保存タイミング\n",
    "    logging_strategy=\"epoch\",  # ロギングのタイミング\n",
    "    evaluation_strategy=\"epoch\",  # 検証セットによる評価のタイミング\n",
    "    load_best_model_at_end=True,  # 訓練後に開発セットで最良のモデルをロード\n",
    "    metric_for_best_model=\"accuracy\",  # 最良のモデルを決定する評価指標\n",
    "    fp16=False,  # 自動混合精度演算の有効化\n",
    "    no_cuda=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_accuracy(\n",
    "    eval_pred: tuple[np.ndarray, np.ndarray]\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"予測ラベルと正解ラベルから正解率を計算\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # predictionsは各ラベルについてのスコア\n",
    "    # 最もスコアの高いインデックスを予測ラベルとする\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2896926e565468eab58b72d5a1fee59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      5\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mencoded_train_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_accuracy,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3324\u001b[0m ):\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/trainer.py:3363\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3362\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3363\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/peft/peft_model.py:1379\u001b[0m, in \u001b[0;36mPeftModelForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[1;32m   1378\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[0;32m-> 1379\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1390\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1392\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:188\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1695\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1693\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1696\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1703\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1709\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    685\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m         output_attentions,\n\u001b[1;32m    692\u001b[0m     )\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:626\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    623\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    624\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 626\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:239\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:638\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 638\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    538\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
