{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '日本で1番高い山はご存知 「富士山」 高さが183メートルです 富士山は、その昔、天慶年間(938)藤原秀郷が富士山から献上されてから 天皇自らが登ったことから'}]\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\", model=\"abeja/gpt2-large-japanese\"\n",
    ")\n",
    "outputs = generator(\"日本で1番高い山は\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.884170</td>\n",
       "      <td>12569</td>\n",
       "      <td>東京</td>\n",
       "      <td>日本 の 首都 は 東京 で ある</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024820</td>\n",
       "      <td>12759</td>\n",
       "      <td>大阪</td>\n",
       "      <td>日本 の 首都 は 大阪 で ある</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020864</td>\n",
       "      <td>13017</td>\n",
       "      <td>京都</td>\n",
       "      <td>日本 の 首都 は 京都 で ある</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  token token_str           sequence\n",
       "0  0.884170  12569        東京  日本 の 首都 は 東京 で ある\n",
       "1  0.024820  12759        大阪  日本 の 首都 は 大阪 で ある\n",
       "2  0.020864  13017        京都  日本 の 首都 は 京都 で ある"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\", model=\"cl-tohoku/bert-base-japanese-v3\"\n",
    ")\n",
    "masked_text = \"日本の首都は[MASK]である\"\n",
    "outputs = fill_mask(masked_text)\n",
    "display(pd.DataFrame(outputs[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.683934</td>\n",
       "      <td>23845</td>\n",
       "      <td>素晴らしい</td>\n",
       "      <td>今日 の 映画 は 刺激 的 で 面白かっ た 。 この 映画 は 素晴らしい 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.101234</td>\n",
       "      <td>24683</td>\n",
       "      <td>面白い</td>\n",
       "      <td>今日 の 映画 は 刺激 的 で 面白かっ た 。 この 映画 は 面白い 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.048003</td>\n",
       "      <td>26840</td>\n",
       "      <td>楽しい</td>\n",
       "      <td>今日 の 映画 は 刺激 的 で 面白かっ た 。 この 映画 は 楽しい 。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  token token_str                                   sequence\n",
       "0  0.683934  23845     素晴らしい  今日 の 映画 は 刺激 的 で 面白かっ た 。 この 映画 は 素晴らしい 。\n",
       "1  0.101234  24683       面白い    今日 の 映画 は 刺激 的 で 面白かっ た 。 この 映画 は 面白い 。\n",
       "2  0.048003  26840       楽しい    今日 の 映画 は 刺激 的 で 面白かっ た 。 この 映画 は 楽しい 。"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "masked_text = \"今日の映画は刺激的で面白かった。この映画は[MASK]。\"\n",
    "outputs = fill_mask(masked_text)\n",
    "display(pd.DataFrame(outputs[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '徳川家康'}]\n"
     ]
    }
   ],
   "source": [
    "t2t_generator = pipeline(\n",
    "    \"text2text-generation\", model=\"retrieva-jp/t5-large-long\"\n",
    ")\n",
    "masked_text = \"江戸幕府を開いたのは、<extra_id_0>である\"\n",
    "outputs = t2t_generator(masked_text, eos_token_id=32098)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本銀行\n"
     ]
    }
   ],
   "source": [
    "masked_text = \"日本で通貨を発行しているのは、<extra_id_0>である\"\n",
    "outputs = t2t_generator(masked_text, eos_token_id=32098)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"日本銀行\" in t2t_generator.tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t2t_generator.tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語とその頻度\n",
    "word_freqs = {\n",
    "    \"たのしい\": 6,\n",
    "    \"たのしさ\": 2,\n",
    "    \"うつくしい\": 4,\n",
    "    \"うつくしさ\": 1,\n",
    "}\n",
    "# 語彙を文字で初期化\n",
    "vocab = sorted(set([char for word in word_freqs for char in word]))\n",
    "# 単語とその分割状態\n",
    "splits = {word: [char for char in word] for word in word_freqs}\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'たのしい': ['た', 'の', 'し', 'い'],\n",
       "  'たのしさ': ['た', 'の', 'し', 'さ'],\n",
       "  'うつくしい': ['う', 'つ', 'く', 'し', 'い'],\n",
       "  'うつくしさ': ['う', 'つ', 'く', 'し', 'さ']},\n",
       " ['い', 'う', 'く', 'さ', 'し', 'た', 'つ', 'の'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits,  vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_most_frequent_pair(\n",
    "    splits: dict[str, list[str]]\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    最も頻度の高い隣接するサブワードの組を計算する\n",
    "    \"\"\"\n",
    "    pair_freqs = Counter()  # サブワードの組のカウンタ\n",
    "    for word, freq in word_freqs.items():  # すべての単語を処理\n",
    "        split = splits[word]  # 現在の単語の分割状態を取得\n",
    "        # すべての隣接したサブワードの組を処理\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            # サブワードの組の頻度に単語の頻度を加算\n",
    "            pair_freqs[pair] += freq\n",
    "    # カウンタから最も頻度の高いサブワードの組を取得\n",
    "    pair, _ = pair_freqs.most_common(1)[0]\n",
    "    return pair\n",
    "\n",
    "def merge_pair(\n",
    "    target_pair: tuple[str, str], splits: dict[str, list[str]]\n",
    ") -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    サブワードの組を結合する\n",
    "    \"\"\"\n",
    "    l_str, r_str = target_pair\n",
    "    for word in word_freqs:  # すべての単語を処理\n",
    "        split = splits[word]  # 現在の単語の分割状態を取得\n",
    "        i = 0\n",
    "        # すべての隣接したサブワードの組を処理\n",
    "        while i < len(split) - 1:\n",
    "            # サブワードの組が結合対象と一致したら結合\n",
    "            if split[i] == l_str and split[i + 1] == r_str:\n",
    "                split = split[:i] + [l_str + r_str] + split[i + 2 :]\n",
    "            i += 1\n",
    "        splits[word] = split  # 現在の結合状態を更新\n",
    "    return splits\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(9):\n",
    "    # 最も頻度の高い隣接するサブワードの組を計算\n",
    "    target_pair = compute_most_frequent_pair(splits)\n",
    "    # サブワードの組を結合\n",
    "    splits = merge_pair(target_pair, splits)\n",
    "    # 語彙にサブワードの組を追加\n",
    "    vocab.append(target_pair)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['い', 'う', 'く', 'さ', 'し', 'た', 'つ', 'の', ('し', 'い'), ('た', 'の'), ('たの', 'しい'), ('う', 'つ'), ('うつ', 'く'), ('うつく', 'しい'), ('し', 'さ'), ('たの', 'しさ'), ('うつく', 'しさ')]\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['自', '然', '言', '語', '処', '理']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "mbert_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\"\n",
    ")\n",
    "\n",
    "print(mbert_tokenizer.tokenize(\"自然言語処理\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['自', '然', '言', '語', '処', '理', 'に', '##ディ', '##ープ', '##ラー', '##ニング', '##を', '使', 'う']\n"
     ]
    }
   ],
   "source": [
    "print(mbert_tokenizer.tokenize(\"自然言語処理にディープラーニングを使う\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '自然', '言語', '処理', 'に', 'ディー', 'プラ', 'ー', 'ニング', 'を使う']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "print(xlmr_tokenizer.tokenize(\"自然言語処理にディープラーニングを使う\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['自然', '言語', '処理', 'に', 'ディープ', 'ラー', '##ニング', 'を', '使う']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satsuki/github/tech-notes/llm-book/src/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bert_ja_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"cl-tohoku/bert-base-japanese-v3\"\n",
    ")\n",
    "print(\n",
    "    bert_ja_tokenizer.tokenize(\"自然言語処理にディープラーニングを使う\")\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
