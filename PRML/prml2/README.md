# 下巻まとめ

## 6章  
https://www.slideshare.net/KeisukeSugawara/slide0629  
https://www.slideshare.net/yukaraikemiya/6-15415589  
松尾研  
https://www.slideshare.net/matsuolab/prml6  

３、４章のパラメトリックモデルでは、訓練データは重みを学習したら破棄される。非線形、複雑なモデルに対応できない。  
カーネル関数：元の次元から特徴空間次元に移して内積をとる関数＝２変数の類似度を定めるようなもの  
内積をとる理由：  
特徴空間は高次元なため、直接計算できず、カーネル関数を通せば簡単に計算できる。多くの線形モデルは双対表現で表すと、カーネル関数が自然に現れる、  

### 6-1
線形回帰モデルの予測関数はカーネル関数を用いた双対表現で表せる。  
利点：全てがカーネル関数で表現されるので、特徴ベクトルφを明示的に考える必要がなくなる。  

### 6-2
関数kが有効なカーネル＝グアム行列Kが半正定値  
多項式カーネル、ガウスカーネル、生成モデルによるカーネル、シグモイドカーネル  

### 6-3
基底関数φは一般的には、RBF（動径基底関数）を使用する　 
RBFの理由：正確に目的変数の値を再現する関数補完、入力変数にノイズが含まれる場合の補完  
RBFは正規化されている。→全ての規定関数が小さい値を持つ領域をなくす。  

#### 6-3-1
訓練集合{x,t}の同時分布p(x,t)の推定にParzen推定法を用いる。  

### 6-4
ガウス過程についての節  
これまで：回帰のための非線形モデルからカーネル方を導いた  
これから；ベイズ的な設定でも自然とカーネルが現れることをみる  
ガウス過程とは、関数y(x)上の確率分布で{x1,x2,,,xn)に対して{y(x1), y(x2),,}の同時分布がガウス分布に従うこと  
ガウス過程の考え方：パラメータwの事前分布p(w)を使うのではなく、関数y(x)の事前分布p(y)を直接定義してしまう。  

#### 6-4-1
線形回帰モデルを再び考える  
wの事前分布を考え、その線形結合であるy自身もガウス分布に従う。平均と分散が求まれば、yの同時分布が定まる。  

#### 6-4-2
ガウス過程では、同時分布が平均、共分散といぅ二時の統計量で記述される。カーネル関数で計算される  
ガウス過程の利点：無限個の規定関数に対応  
欠点：O(N^3)の計算量を必要とする、逆行列の計算が必要。  

#### 6-4-3
ガウス過程による予測は共分散関数の選択に依存  
パラメトリックな関数族を考えて、そのパラメータθをデータから推測。  
超パラメータの学習方法：  
尤度関数p(t|θ)を評価、簡単な方法は対数尤度関数を最大化するθの点推定を行う。  
尤度関数の最大化は共役勾配法などの最適化アルゴリズムを用いる。  

#### 6-4-5
確率的な手法による分類：区間（0,1）に収まる事後分布を求める  
ガウス過程のモデルは予測が実数値全体での値をとる  
→ガウス過程の出力を非線形な活性化関数で変換し、ガウス過程を分類問題に適用できるようにする  
2クラス分類問題の場合：ロジスティックシグモイド関数で変換  
目的：予測分布p(tn+1|t)の決定  
事後分布をガウス分布による近似。  
近似方法：変分推論法、EP法、ラプラス近似  

#### 6-4-6
ラプラス近似についての節  
目的はp(a|t)のラプラス近似(ガウス分布に近似)して、積分をしやすくする  
ラプラス近似の流れ  
1.ラプラス近似の式を導出  
2.もとの確率分布のモードを求める。→ニュートンラフソン法  
3.モードにおいてヘッセ行列を評価  

#### 6-4-5
ニューラルネットワークの隠れユニット数Mを無限大に極限をとると、ガウス過程に近く。  

## 7章
https://www.slideshare.net/tsukasafukunaga5/prml07-17444396  
https://www.slideshare.net/thorikawa/prml-chapter7  

松尾研  
https://www.slideshare.net/matsuolab/prml7-78266098  

カーネル法：カーネル関数を用いることで、特徴空間の写像を明示的に構成する必要がなくなる（カーネルトリック）  
カーネル関数k(x,x)を全ての訓練データ対x,xについて計算する必要があるため、学習に時間がかかる。  
→疎な解を持ち、訓練データの一部だけを用いてカーネル関数を計算することで新しい入力の予測ができるSVM,RVMを扱う  

### 7-1
訓練データを超平面を分離境界として分類する  
サポートベクトルとの距離（マージン）を最大化する分類平面を選ぶ  
まずは線形モデルで二値分類モデルを解く  
SVMでマージンを最大化する理由は、計算論的学習理論と、統計的学習理論で説明できる  
パーセプトロンは、有限の計算ステップで解を見つけることが保証される。解はパラメータの初期値、データの入力順番による  
SVMのマージン最大化には、二次最適化問題を解けばいい  
制約付き最適化問題をラグランジュ未定乗数法により決定する。  
学習したモデルを用いて新しいデータ点を分類するには、KKT条件のもとで分類する。  

#### 7-1-1
これまでは、線形分離可能な場合を考えたが、不可能な場合を考える。  
スラック変数を導入する。  
スラック変数の和は誤分類されたデータ数の上界  

#### 7-1-2
SVMとロジスティック回帰の関係を説明
SVMの誤差関数はロジスティック回帰と似た形状。  
SVMは外れ値に弱い。  

#### 7-1-3
他クラスSVMについての節  
１対他方式：全てのクラスの組み合わせについて２クラスSVMを学習して、最も多くの分類器の正例として、投票して決める。  
DAGSVM:有向非巡回グラフで表現される順で分類器を適用  
誤り訂正出力符号：クラス集合の分割方法を適切に設計することで、精度をあげられる。  

#### 7-1-4
SVMの線形回帰問題で二乗誤差関数をε許容誤差関数で置き換える。  
分類問題と同様にスラック変数を導入して最適化問題として表現。  

#### 7-1-5
SVMは計算論的学習理論から生まれた。  
PAC学習の目的：いい汎化性能を出すために、どれくらいの数の学習データが必要か求める  
VC次元：関数空間の複雑さを示す量  

#### 7-2-1
関連ベクトルマシン(RVM)：疎なカーネルベースのベイズ流学習手法  
RVM回帰はベイズ線形回帰とほぼ同じ。事前分布を重みパラメータに採用する。  
パラメータの事後分布がゼロ１点となり、モデルから取り除けるため疎なモデルができる。  
ゼロでない重みを持つ基底関数に対応する入力は関連ベクトルと呼ぶ  

  
## 8章
グラフィカルモデル解説  
https://www.slideshare.net/Kawamoto_Kazuhiko/ss-35483453   
8章の詳しい解説  
https://www.slideshare.net/antiplastics/prml8   
8.2解説
https://www.slideshare.net/sleepy_yoshi/prml-82  

松尾研  
https://www.slideshare.net/matsuolab/prml8-78266113  

確率論は、加法定理と乗法定理から成り立ち、複雑な確率モデルも二つの等式から導ける。  
グラフィカルモデル：確率変数間の依存関係をグラフで表現  
グラフィカルモデルの利点  
1,確率モデルの視覚化ができ、新しいモデルの設計指針を決めるのに役立つ  
2,グラフ構造から、条件付き独立性などのモデルの性質についての知見が得られる。  
3,複雑な計算を数学的な表現をグラフ上の操作として表現できる。  

ベイジアンネットワーク：グラフのリンクが特定の方向性を持ち、矢印で描かれる。物体追跡など、系列データに応用  
マルコフ確率場：無効グラフィカルモデル。リンクが方向性を持たない。画像復元など、因果関係が自明でないデータに応用

### 8-1
同時確率分布は乗法定理で分解可能。　　

　　　
   
## 9章
解説スライド  
https://www.slideshare.net/takao-y/20131113-em  
https://www.slideshare.net/KeisukeSugawara/prml-em  
松尾研  
https://www.slideshare.net/matsuolab/prml8-78266113  


## １０章  
須山さん解説ブログ
http://machine-learning.hatenablog.com/entry/2016/01/23/123033  
解説スライド  
https://www.slideshare.net/takao-y/ss-28872465  
https://www.slideshare.net/ruto5/prml14th-107  
松尾研  
https://www.slideshare.net/matsuolab/prml10-78266202  


## 11章

